{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home assigment von Alfons Dauer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.read_csv(\"validation.csv\")\n",
    "train = pd.read_csv(\"training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3700, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take a first look into the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>v1;\"v2\";\"v3\";\"v4\";\"v5\";\"v6\";\"v7\";\"v8\";\"v9\";\"v10\";\"v11\";\"v12\";\"v13\";\"v14\";\"v15\";\"v17\";\"v18\";\"v19\";\"classLabel\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a;17</th>\n",
       "      <th>92;5</th>\n",
       "      <th>4e-05;\"u\";\"g\";\"c\";\"v\";1</th>\n",
       "      <td>75;\"f\";\"t\";1;\"t\";\"g\";80;5;8e+05;\"t\";0;\"no.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b;16</th>\n",
       "      <th>92;3</th>\n",
       "      <th>35e-05;\"y\";\"p\";\"k\";\"v\";0</th>\n",
       "      <td>29;\"f\";\"f\";0;\"f\";\"s\";200;0;2e+06;NA;0;\"no.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b;31</th>\n",
       "      <th>25;0</th>\n",
       "      <th>0001125;\"u\";\"g\";\"ff\";\"ff\";0;\"f\";\"t\";1;\"f\";\"g\";96;19;960000;\"t\";0;\"no.\"</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a;48</th>\n",
       "      <th>17;0</th>\n",
       "      <th>0001335;\"u\";\"g\";\"i\";\"o\";0</th>\n",
       "      <td>335;\"f\";\"f\";0;\"f\";\"g\";0;120;0;NA;0;\"no.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b;32</th>\n",
       "      <th>33;0</th>\n",
       "      <th>00035;\"u\";\"g\";\"k\";\"v\";0</th>\n",
       "      <td>5;\"f\";\"f\";0;\"t\";\"g\";232;0;2320000;\"f\";0;\"no.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a;34</th>\n",
       "      <th>83;0</th>\n",
       "      <th>000125;\"y\";\"p\";\"i\";\"h\";0</th>\n",
       "      <td>5;\"f\";\"f\";0;\"t\";\"g\";160;0;1600000;\"f\";0;\"no.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a;26</th>\n",
       "      <th>17;2e-04;\"u\";\"g\";\"j\";\"j\";0;\"f\";\"f\";0;\"t\";\"g\";276;1;2760000;NA;0;\"no.\"</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b;21</th>\n",
       "      <th>17;8</th>\n",
       "      <th>75e-05;\"y\";\"p\";\"c\";\"h\";0</th>\n",
       "      <td>25;\"f\";\"f\";0;\"f\";\"g\";280;204;2800000;NA;0;\"no.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b;28</th>\n",
       "      <th>92;3</th>\n",
       "      <th>75e-05;\"u\";\"g\";\"c\";\"v\";0</th>\n",
       "      <td>29;\"f\";\"f\";0;\"f\";\"g\";220;140;2200000;NA;0;\"no.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b;18</th>\n",
       "      <th>17;0</th>\n",
       "      <th>001025;\"u\";\"g\";\"c\";\"h\";1</th>\n",
       "      <td>085;\"f\";\"f\";0;\"f\";\"g\";320;13;3200000;NA;0;\"no.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a;24</th>\n",
       "      <th>75;0</th>\n",
       "      <th>0013665;\"u\";\"g\";\"q\";\"h\";1</th>\n",
       "      <td>5;\"f\";\"f\";0;\"f\";\"g\";280;1;2800000;\"f\";0;\"no.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a;31</th>\n",
       "      <th>75;3e-04;\"y\";\"p\";\"j\";\"j\";0;\"f\";\"f\";0;\"f\";\"g\";160;20;1600000;\"f\";0;\"no.\"</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">a;18</th>\n",
       "      <th>17;0</th>\n",
       "      <th>001;\"y\";\"p\";\"q\";\"h\";0</th>\n",
       "      <td>165;\"f\";\"f\";0;\"f\";\"g\";340;0;3400000;\"f\";0;\"no.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25;0</th>\n",
       "      <th>001;\"u\";\"g\";\"W\";\"v\";1;\"f\";\"t\";1;\"f\";\"g\";120;1;1200000;NA;0;\"no.\"</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b;17</th>\n",
       "      <th>58;0</th>\n",
       "      <th>001;\"u\";\"g\";\"W\";\"h\";0</th>\n",
       "      <td>165;\"f\";\"t\";1;\"f\";\"g\";120;1;1200000;\"t\";0;\"no.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                           v1;\"v2\";\"v3\";\"v4\";\"v5\";\"v6\";\"v7\";\"v8\";\"v9\";\"v10\";\"v11\";\"v12\";\"v13\";\"v14\";\"v15\";\"v17\";\"v18\";\"v19\";\"classLabel\"\n",
       "a;17 92;5                                               4e-05;\"u\";\"g\";\"c\";\"v\";1                                   75;\"f\";\"t\";1;\"t\";\"g\";80;5;8e+05;\"t\";0;\"no.\"                                                           \n",
       "b;16 92;3                                               35e-05;\"y\";\"p\";\"k\";\"v\";0                                  29;\"f\";\"f\";0;\"f\";\"s\";200;0;2e+06;NA;0;\"no.\"                                                           \n",
       "b;31 25;0                                               0001125;\"u\";\"g\";\"ff\";\"ff\";0;\"f\";\"t\";1;\"f\";\"g\";9...                                                NaN                                                           \n",
       "a;48 17;0                                               0001335;\"u\";\"g\";\"i\";\"o\";0                                    335;\"f\";\"f\";0;\"f\";\"g\";0;120;0;NA;0;\"no.\"                                                           \n",
       "b;32 33;0                                               00035;\"u\";\"g\";\"k\";\"v\";0                                 5;\"f\";\"f\";0;\"t\";\"g\";232;0;2320000;\"f\";0;\"no.\"                                                           \n",
       "a;34 83;0                                               000125;\"y\";\"p\";\"i\";\"h\";0                                5;\"f\";\"f\";0;\"t\";\"g\";160;0;1600000;\"f\";0;\"no.\"                                                           \n",
       "a;26 17;2e-04;\"u\";\"g\";\"j\";\"j\";0;\"f\";\"f\";0;\"t\";\"g\";27... NaN                                                                                               NaN                                                           \n",
       "b;21 17;8                                               75e-05;\"y\";\"p\";\"c\";\"h\";0                              25;\"f\";\"f\";0;\"f\";\"g\";280;204;2800000;NA;0;\"no.\"                                                           \n",
       "b;28 92;3                                               75e-05;\"u\";\"g\";\"c\";\"v\";0                              29;\"f\";\"f\";0;\"f\";\"g\";220;140;2200000;NA;0;\"no.\"                                                           \n",
       "b;18 17;0                                               001025;\"u\";\"g\";\"c\";\"h\";1                              085;\"f\";\"f\";0;\"f\";\"g\";320;13;3200000;NA;0;\"no.\"                                                           \n",
       "a;24 75;0                                               0013665;\"u\";\"g\";\"q\";\"h\";1                               5;\"f\";\"f\";0;\"f\";\"g\";280;1;2800000;\"f\";0;\"no.\"                                                           \n",
       "a;31 75;3e-04;\"y\";\"p\";\"j\";\"j\";0;\"f\";\"f\";0;\"f\";\"g\";16... NaN                                                                                               NaN                                                           \n",
       "a;18 17;0                                               001;\"y\";\"p\";\"q\";\"h\";0                                 165;\"f\";\"f\";0;\"f\";\"g\";340;0;3400000;\"f\";0;\"no.\"                                                           \n",
       "     25;0                                               001;\"u\";\"g\";\"W\";\"v\";1;\"f\";\"t\";1;\"f\";\"g\";120;1;1...                                                NaN                                                           \n",
       "b;17 58;0                                               001;\"u\";\"g\";\"W\";\"h\";0                                 165;\"f\";\"t\";1;\"f\";\"g\";120;1;1200000;\"t\";0;\"no.\"                                                           "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a)\n",
    "This doesen't look nice. First things that got my attention:\n",
    "\n",
    "1. The columns have to be seperated from one another.\n",
    "2. There are multiple index and some information is written into the index. Also I do not know if there is in fact a real index or not. I will just interpret everything as a variable. If there is indeed an index somewhere and the data set is ordered it could lead to a problem. I will have to keep that in mind.\n",
    "3. The variable names are non-descriptive. This leads to: \n",
    "    - feature engineering with a priori knowledge is impossible\n",
    "    - we don't know if some variables are rounded and continuous or categorical/ordinal\n",
    "    - we don't know if false positives and false negatives have the same cost\n",
    "    - no way of interpreting the results\n",
    "\n",
    "4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1691"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats a lot of rows with missing values considering we only have 3700 rows to begin with. Trying to recover the missing values could be worth the hassle but for this we need to find out what variables are acutally missing. So i just drop them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us do the same for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(validation.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = validation.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1b) \n",
    "So far i don't find it necessary to remove features since we don't have that many, even if some are probably useless like the one with a bunch of zeros in front. Also one variable has a lot of NAs but since it is categorical i will just remove it after one-hot encoding the available information of that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function to tidy up the data frame\n",
    "\"\"\"\n",
    "def tidy_data(df):\n",
    "    \n",
    "    flat_data = index_to_data(df)\n",
    "    column_names = get_column_names(df)\n",
    "    \n",
    "    data = pd.DataFrame(flat_data, columns = column_names)\n",
    "    data = data.replace('\"', '', regex=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\"\"\"\n",
    "function to combine the data from all the indeces with the row data\n",
    "\"\"\"\n",
    "def index_to_data(df):\n",
    "\n",
    "    # put the data from the index into the columns\n",
    "    temp = []\n",
    "    for i in range(df.shape[0]):\n",
    "        temp.append(list(df.index[i]) + list(df.iloc[i]))\n",
    "        \n",
    "    # flatten data of every row into one list\n",
    "    flat_data = []\n",
    "    for i in range(df.shape[0]):\n",
    "        flat_data.append([x.split(';') for x in temp[i] if str(x) != 'nan'])\n",
    "        flat_data[i] = sum(flat_data[i], [])\n",
    "        \n",
    "    # check if all rows have the same length\n",
    "    len_first = len(flat_data[0]) if flat_data else None\n",
    "    assert all(len(i) == len_first for i in flat_data), \"Not all datapoints have the same number of dimensions after removing na and flattening\"\n",
    "    \n",
    "    return flat_data\n",
    "\n",
    "\"\"\"\n",
    "function to get the column names of our df\n",
    "\"\"\"\n",
    "def get_column_names(df):\n",
    "    column_names = list(df)[0].replace('\"', '').split(\";\")\n",
    "    column_names.insert(15, 'v16')\n",
    "\n",
    "    # I am not sure if the first two var are an index, i first add them as variables and maybe remove them later.\n",
    "    column_names.insert(0, 'var_ind_2')\n",
    "    column_names.insert(0, 'var_ind_1')\n",
    "    \n",
    "    return column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_train = tidy_data(train)\n",
    "tidy_val = tidy_data(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_ind_1</th>\n",
       "      <th>var_ind_2</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v11</th>\n",
       "      <th>v12</th>\n",
       "      <th>v13</th>\n",
       "      <th>v14</th>\n",
       "      <th>v15</th>\n",
       "      <th>v16</th>\n",
       "      <th>v17</th>\n",
       "      <th>v18</th>\n",
       "      <th>v19</th>\n",
       "      <th>classLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>17</td>\n",
       "      <td>92</td>\n",
       "      <td>5</td>\n",
       "      <td>4e-05</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>8e+05</td>\n",
       "      <td>t</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>16</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>35e-05</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>2e+06</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>48</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0001335</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>i</td>\n",
       "      <td>o</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>00035</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>2320000</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>34</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>000125</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>i</td>\n",
       "      <td>h</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>1600000</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>no.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  var_ind_1 var_ind_2  v1 v2       v3 v4 v5 v6 v7 v8    ...     v11 v12 v13  \\\n",
       "0         a        17  92  5    4e-05  u  g  c  v  1    ...       t   1   t   \n",
       "1         b        16  92  3   35e-05  y  p  k  v  0    ...       f   0   f   \n",
       "2         a        48  17  0  0001335  u  g  i  o  0    ...       f   0   f   \n",
       "3         b        32  33  0    00035  u  g  k  v  0    ...       f   0   t   \n",
       "4         a        34  83  0   000125  y  p  i  h  0    ...       f   0   t   \n",
       "\n",
       "  v14  v15  v16      v17 v18 v19 classLabel  \n",
       "0   g   80    5    8e+05   t   0        no.  \n",
       "1   s  200    0    2e+06  NA   0        no.  \n",
       "2   g    0  120        0  NA   0        no.  \n",
       "3   g  232    0  2320000   f   0        no.  \n",
       "4   g  160    0  1600000   f   0        no.  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidy_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can work with our classification models we transform the data to numpy arrays and one-hot encode the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function to transform the df into a matrix we can work with.\n",
    "\"\"\"\n",
    "def df_to_matrix(df, categorical_variables): # categorical_variables is a list of column names that are categorical/strings\n",
    "\n",
    "    dummy_var = dummy_encode(df, categorical_variables)\n",
    "    df = df.drop(categorical_variables, axis = 1)\n",
    "    \n",
    "    DV, IV = get_IV_DV(df, dummy_var)\n",
    "    \n",
    "    return DV, IV\n",
    "\n",
    "\"\"\"\n",
    "function to dummy encode the categorical variables\n",
    "\"\"\"\n",
    "def dummy_encode(df, categorical_variables):\n",
    "    \n",
    "    dummy_var = pd.get_dummies(df[categorical_variables], drop_first = True)\n",
    "    dummy_var = dummy_var.as_matrix()\n",
    "    \n",
    "    return dummy_var\n",
    "\n",
    "\"\"\"\n",
    "function to split the data into the independent variables and dependent variable. The DV has to be the last dummy_var\n",
    "\"\"\"\n",
    "def get_IV_DV(numeric_var, dummy_var):\n",
    "    \n",
    "    DV = dummy_var[:, dummy_var.shape[1]-1]\n",
    "    IV_dummies = dummy_var[:, :-1]\n",
    "    \n",
    "    # scale the numeric variables\n",
    "    numeric_var = numeric_var.as_matrix()\n",
    "    scaler = StandardScaler()\n",
    "    IV_numeric = scaler.fit_transform(numeric_var.astype(float))\n",
    "    \n",
    "    IV = np.c_[IV_numeric, IV_dummies]\n",
    "    \n",
    "    return DV, IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\praktikum\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\User\\Anaconda3\\envs\\praktikum\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "#drop NAs except if NA is in v18\n",
    "tidy_train = tidy_train[~tidy_train.drop('v18', axis = 1).eq('NA').any(1)]\n",
    "tidy_val = tidy_val[~tidy_val.drop('v18', axis = 1).eq('NA').any(1)]\n",
    "\n",
    "concatenation_point = tidy_train.shape[0]\n",
    "tidy_data = pd.concat([tidy_train, tidy_val])\n",
    "\n",
    "categorical_variables = ['var_ind_1', 'v4', 'v5', 'v6', 'v7', 'v10', 'v11', 'v13', 'v14', 'v18', 'classLabel']\n",
    "\n",
    "DV, IV = df_to_matrix(tidy_data, categorical_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DV_train = DV[:concatenation_point]\n",
    "IV_train = IV[:concatenation_point,:]\n",
    "\n",
    "DV_val = DV[concatenation_point:]\n",
    "IV_val = IV[concatenation_point:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=uint8), array([ 215, 1862], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(DV, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are very unbalanced. There are a couple ways we can deal with this. I try simple oversampling first. I have to be kind of careful because we only have 147 data points in one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "IV_resampled, DV_resampled = ros.fit_resample(IV_train, DV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use a logistic regression as a first model to try\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty = 'l2', random_state=0, solver = 'lbfgs').fit(IV_resampled, DV_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(IV_resampled, DV_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48360655737704916"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(IV_val, DV_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.01008946e-01, -4.63490473e-02,  2.60282746e-02,  7.25042623e-02,\n",
       "        1.89729840e-01,  2.36480978e-02,  2.68788655e-01, -4.24408077e-02,\n",
       "        1.76965759e-01, -4.24408077e-02,  4.14705072e+00, -8.22541817e-02,\n",
       "       -6.31610237e-02, -6.31610237e-02, -4.17433449e-03, -9.88853511e-03,\n",
       "        8.96878600e-02, -2.02161131e-02, -8.30356239e-03, -4.16286371e-02,\n",
       "       -6.64595905e-02, -5.19156332e-03, -7.76353502e-02, -2.31835707e-02,\n",
       "        3.83133120e-02,  5.50318193e-03,  1.23797259e-01, -4.16286371e-02,\n",
       "        3.19769916e-02,  2.48699347e-02,  1.54752846e-03, -1.10846650e-02,\n",
       "        1.72016519e-03,  0.00000000e+00,  5.69505511e-01,  1.58727656e-01,\n",
       "       -1.13004179e-02, -7.23177785e-03,  1.88970962e-03, -1.29139299e-01,\n",
       "        4.21887644e-02])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 41 artists>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "plt.pyplot.bar(range(len(lr.coef_[0])), lr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hopelessly overfit on the training data. Plotting the coefficients we see that the model is basically only looking at the variable 'v10'. Unfortunately, the connection between v10 and classLabel only exists in the training data. So I drop the variable and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3616, 41)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IV_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "IV_resampled = np.delete(IV_resampled, 10, 1)\n",
    "IV_val = np.delete(IV_val, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8896570796460177"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty = 'l2', random_state=0, solver = 'lbfgs').fit(IV_resampled, DV_resampled)\n",
    "lr.score(IV_resampled, DV_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8934426229508197"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(IV_val, DV_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! it seems that removing that feature was all we needed to do to get a working model. We get an accuracy of 89% on our validation set. Comparing it to the accuracy in our training set suggests that we don't overfit.\n",
    "\n",
    "Of course accuracy is not the best metric in our unbalanced data set. For a better assessement let us look at the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60,  8],\n",
       "       [ 5, 49]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred_val = lr.predict(IV_val)\n",
    "confusion_matrix(DV_val, pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio seems decent. We get a precision of 0.88 and a recall of 0.92.\n",
    "\n",
    "With more time i could try other things to get beter predictions.\n",
    "Now let us see the new coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 40 artists>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pyplot.bar(range(len(lr.coef_[0])), lr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbesserungsmöglichkeiten\n",
    "(nach Priorität geordnet)\n",
    "### Predictive Modeling:\n",
    "\n",
    "- Modell anpassen, falls False Positives und False Negatives verschieden viele Kosten verursachen.\n",
    "- mehr Modelle/mehr Hyperparametertuning wie z. B. tree based models, SVMs. In diesem Fall würden wir mehrere Modelle testen und brauchen dann neben dem Validationsset noch ein Testset.\n",
    "- Besser mit den NA Werten umgehen, anstatt die entsprechenden Zeilen einfach aus dem Modell zu schmeißen. Z.B. anhand der Sequenz herausfinden, welche Variablen in jeder Zeile fehlen und dann evtl. entsprechende Variablen rausschmeißen oder versuchen NaNs zu predicten\n",
    "- feature Engineering: dafür wäre vor allem ein Verständnis für die einzelnen Variablen nützlich. Also irgendwie in Erfahrung bringen, was die Variablen eigentlich repräsentieren.\n",
    "- Weitere Metriken neben Genauigkeit zur Evaluation des Algorithmus heranziehen. Zum Beispiel ROC-AUC. Das ist nicht so mega wichtig mMn, weil die Confusion Matrix uns doch recht solide Ergebnisse gezeigt hat.\n",
    "- Anders mit den Unbalancierten Klassen umgehen. Z.B. SMOTE, ROC oder kostensensitive Lossfunktionen. Letzteres ist auch nützlich falls False Positives und False Negatives mit verschieden viel Kosten verbunden sind\n",
    "\n",
    "### Data Pipeline:\n",
    "- Pipeline verallgemeinern: Falls man erwarten kann, dass man immer mal wieder Datensets mit ähnlicher Struktur erhält vereinfacht automatisiertes Data Cleaning die Arbeit erheblich. Zurzeit funktioniert das nur für dieses spezielle Datenset.\n",
    "- Unit Tests, Assert Lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
